---
title: VAE：变分自编码学习笔记
categories:
- 生成模型
tags:
- VAE
updated: 2018-10-24
---
![](/assets/blog_images/2018-10-24-title.png)

### 数学推导
VAE跟普通的自编码器差别不大，无非是多加了噪声并对噪声做了约束，使得隐变量$Z$符合标准正态分布从而得到一个较为理想的生成模型$(px|z)$，自动编码器隐变量$Z$的分布是未知的，因此并不能采样得到新样本的能力。通常求一个数据集的分布时，可以利用最大似然法来估算学习的参数$\Theta$:
![](/assets/blog_images/2018-10-24-公式1.png)
从上面的似然函数函数引出隐变量$z$，推导似然函数的下界$E_{z\sim q}[\log p(x|z)]-D_{KL}(q(z|x)||p(z)$：
![](/assets/blog_images/2018-10-24-公式2.png)
![](/assets/blog_images/2018-10-24-公式3.png)
最大化下界$E_{z\sim q}[\log p(x|z)]-D_{KL}(q(z|x)||p(z)$也就是最小化$L$：
![](/assets/blog_images/2018-10-24-公式4.png)


### 算法实现
![VAE的算法流程图](/assets/blog_images/2018-10-24-flowchart.png)
- Encoder模块
由两个输入为n维，输出为m维的神经网络组成，分别计算正态分布$p(z|x)$的均值网络和方差网络
	```
	batch_size = 100
	original_dim = 784  # 28*28
	latent_dim = 2
	intermediate_dim = 256
	nb_epoch = 50
	epsilon_std = 1.0
	
	x = Input(batch_shape=(batch_size, original_dim))
	h = Dense(intermediate_dim, activation='relu')(x)
	z_mean = Dense(latent_dim)(h)
	z_log_var = Dense(latent_dim)(h)
	```
- 采样模块
	```
	def sampling(args):
	    z_mean, z_log_var = args
	    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0, stddev=epsilon_std)
	    return z_mean + K.exp(z_log_var / 2) * epsilon
	
	z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])
	```

- Decoder模块
一个输入为m维，输出为n维的神经网络
	```
	decoder_h = Dense(intermediate_dim, activation='relu')
	decoder_mean = Dense(original_dim, activation='sigmoid')
	h_decoded = decoder_h(z)
	x_decoded_mean = decoder_mean(h_decoded)
	```

- 优化目标
	- 希望 $\hat{x}$与$x$自身尽量的接近，$x$经过编码(Encoder)后，能够通过解码(Decoder)尽可能多的恢复出原来的信息，由交叉熵xent或者均方误差mse来测量  
	$$xent=\sum_{i=1}^n-[x_i\cdot\log(\hat{x}_i)+(1-x_i)\cdot\log(1-\hat{x}_i)]$$
    $$xent=\sum_{i=1}^n(x_i-\hat{x}_i)^2$$
	- 希望原始数据$X$对应的隐变量空间$Z$为一个标准正态分布，让所有的$p(z|x)$向“标准正态分布”看齐，则$p(z|x)$与$N(0,I)$的KL散度越小越好
	$$KL=-0.5*(1+\log\sigma^2-\mu-exp(log\sigma^2))$$
	- 最终的损失函数：  
	$$loss=xent+KL \quad or\quad loss=mse+KL$$  
	```
	def vae_loss(x, x_decoded_mean):
	    # xloss = original_dim * objectives.binary_crossentropy(x, x_decoded_mean)
	    xloss = original_dim * objectives.mean_squared_error(x, x_decoded_mean)
	    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)
	    return xloss + kl_loss
	
	vae = Model(x, x_decoded_mean)
	vae.summary()
	vae.compile(optimizer='rmsprop', loss=vae_loss)
	
	```

### VAE应用
- 数据生成
由于指定$p(z)$是标准正态分布，结合已训练好的$p(x|z)$，随机在标准正态分布下采样，就可以生成类似但不同于训练数据集的新样本

![](/assets/blog_images/2018-10-24-generation.png)

- 高维数据可视化
enconder将数据$x$映射到更低维的$z$空间（二维或者三维）就可以直观地展示出来
![](/assets/blog_images/2018-10-24-visualization.png)

- 缺失数据填补
对许多现实问题，样本点的各维数据存在相关性。因此，在部分维度缺失或不准确的情况，有可能通过相关信息得到填补
![](/assets/blog_images/2018-10-24-imputation.png)

- 半监督学习或者无监督学习
相比于高成本的有标注的数据，无标注数据更容易获取。半监督学习试图只用一小部分有标注的数据加上大量无标注数据，来学习到一个较好预测模型（分类或回归）。 VAE 是无监督的，而且也可以学习到较好的特征表征，因此，可以被用来作无监督学习

参考：  
[ 变分自编码器（一）：原来是这么一回事](https://kexue.fm/archives/5253/comment-page-2#comments)
[变分自编码器](https://blog.csdn.net/jackytintin/article/details/53641885)
[VAE背后的哲学思想及数学原理](https://blog.csdn.net/witnessai1/article/details/78532193)






